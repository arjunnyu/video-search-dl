{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJhTIUkxV-yW"
   },
   "source": [
    "# CSGY 6953 Deep Learning Final Project\n",
    "In this project, we will implement a dual-encoder model for image search. In particular, our model will be fed a text query and will return several images that are related to the query. To do this task, our model will be trained so that it embeds both image and text data into the same space, and importantly, encodes relevant data to be close each other in the embedding space. This will be done by developing two encoders, one for image processing and the other for text encoding, and training them by a similarity-based loss function. <br>\n",
    "<br>\n",
    "In this notebook, I develop the evaluation metrics, median rank (**MedR**) and recall rate at top K (**R@K**). <br>\n",
    "<br>\n",
    "In the test phase, we first project all the test images into the embedding space via the image encoder. For each test (image, text) pair, we consider the text as a query, which is also mapped into the embedding space through the text encoder. Then, we rank all the test images by the similarity to the query. The **MedR** is the median of the rank of the true image over all the test data. So, smaller **MedR** indicates that the model works well. In **R@K**, we take only top K images for each query and check if the true image is included. By doing this check for all test data, we can calculate the probability that the true image is successfully retrieved. Thus, higher **R@K** is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_yx06Sfl22Hc"
   },
   "source": [
    "### Intuition behind the CosineEmbeddingLoss\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html<br>\n",
    "$$\\text{loss}(x,y) = \\begin{cases} 1-\\cos(x_{image}, x_{text}) & \\text{if } y=1 \\\\ \\max(0, \\cos(x_{image}, x_{text})- \\text{margin}) & \\text{if } y=-1 \\end{cases}$$\n",
    "Let $d$ be the dimensionality of the embedding space. When negative pair $(x^-, -1)$ is given, we want to map $x_{image}, x_{text}$ in distant place. One might think that having $\\cos(x_{image}, x_{text}) = -1$ is ideal, but given $x_{image}$, there is only one vector (up to scaling) in the embedding space that satisfies it. Since we have many vectors that are separated each other, forcing all the pairs of them having cosine similarity (nearly) $-1$ is impossible. <br>\n",
    "By contrast, forcing them having cosine similarity (nearly) $0$ is possible. In other words, forcing them being (nearly) orthogonal is possible. Nearly orthogonal is defined as the normalized dot product is $0$ with a small error term $\\epsilon > 0$. That is, \n",
    "$$-\\epsilon \\leq \\frac{\\langle x_i, x_j \\rangle}{\\lVert x_i \\rVert_2 \\lVert x_j \\rVert_2} \\leq \\epsilon$$\n",
    "While there are only $d$ mutually orthogonal vectors in $d$ dimensional space, there exists $2^{\\Theta(\\epsilon^2 d)}$ mutually nearly orthogonal vectors. Thus, our goal is to train encoders so that negative inputs are mapped as nearly orthogonal. In this context, it makes sense not to penalize so long as cosine similarity is negative, and moreover, allow a small margin to let vectors nearly orthogonal, not strictly orthogonal. <br>\n",
    "Note that for positive pairs, this loss function enforces them to having cosine similarity $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "W1jCGXeDVVSM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import tqdm\n",
    "from yolov5.models import yoloE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebQuixn6ZKEk",
    "outputId": "0a93a315-ee70-4e0f-ff86-646787d464c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RiD9kidTZLVH"
   },
   "outputs": [],
   "source": [
    "with open(\"./../dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.uint8"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset['images'][0][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "C0OMHApOozeC",
    "outputId": "042a5448-1a6d-4e32-dd2d-52be90b45801"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shortest caption length: 6\n",
      "Longest caption length: 46\n",
      "The maximum sentence length for the text encoder is set to 55.\n"
     ]
    }
   ],
   "source": [
    "text_len = [len(s.split()) for s in dataset[\"texts\"]]\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.xlabel(\"caption length\")\n",
    "plt.ylabel(\"# captions\")\n",
    "plt.hist(text_len)\n",
    "plt.show()\n",
    "print(\"Shortest caption length: \" + str(min(text_len)))\n",
    "print(\"Longest caption length: \" + str(max(text_len)))\n",
    "\n",
    "max_len = 55 # BERT tokenizer is not one word per one embedding.\n",
    "print(\"The maximum sentence length for the text encoder is set to \" + str(max_len) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgDpODirZScy",
    "outputId": "63f980e3-85f9-4075-888a-f81e8b68c185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:  3150\n",
      "Training texts:  31500\n",
      "Test images:  1350\n",
      "Test texts:  13500\n"
     ]
    }
   ],
   "source": [
    "train_pct = 0.7\n",
    "threshold = int(len(dataset[\"images\"]) * train_pct)\n",
    "train_images = dataset[\"images\"][:threshold]\n",
    "train_texts = dataset[\"texts\"][:threshold * 10]\n",
    "test_images = dataset[\"images\"][threshold:]\n",
    "test_texts = dataset[\"texts\"][threshold * 10:]\n",
    "print(f\"Training images: \", len(train_images))\n",
    "print(f\"Training texts: \", len(train_texts))\n",
    "print(f\"Test images: \", len(test_images))\n",
    "print(f\"Test texts: \", len(test_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kaBGEWLpZTsi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "wv2zQDXTZYpi"
   },
   "outputs": [],
   "source": [
    "# Reference: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
    "\n",
    "class DualEncoderDataset(Dataset):\n",
    "    def __init__(self, images, texts, neg_rate=4):\n",
    "        # This class assumes that num_image < num_texts, and the number of texts per image is the same for all images.\n",
    "        self.imagedata = images\n",
    "        self.textdata = texts\n",
    "        self.num_images = len(images)\n",
    "        self.num_texts = len(texts)\n",
    "        self.image_text_rate = int(self.num_texts / self.num_images)\n",
    "        self.num_negative = int(self.num_texts * neg_rate)\n",
    "        self.random_map = []\n",
    "        self.shuffle()\n",
    "    \n",
    "    def shuffle(self):\n",
    "        self.random_map = np.random.randint(0, self.num_texts - self.image_text_rate, self.num_negative)\n",
    "        for i in range(self.num_negative):\n",
    "            if self.random_map[i] >= i % self.num_texts // self.image_text_rate * self.image_text_rate:\n",
    "                self.random_map[i] += self.image_text_rate\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_texts + self.num_negative\n",
    "\n",
    "    def __getitem__(self, sample_idx):\n",
    "        if torch.is_tensor(sample_idx):\n",
    "            sample_idx = sample_idx[0].tolist()\n",
    "        image_idx = sample_idx % self.num_texts // self.image_text_rate\n",
    "        if sample_idx < self.num_texts:\n",
    "            text_idx = sample_idx\n",
    "        else:\n",
    "            text_idx = self.random_map[sample_idx - self.num_texts]\n",
    "        sample = {\"images\": self.imagedata[image_idx], \"texts\": self.textdata[text_idx], \"labels\": int(sample_idx < self.num_texts) * 2 - 1}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJvcxfAnZcWC",
    "outputId": "64977934-e299-4efb-a421-09736151f2cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is  cpu\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(\"The device is \",device.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvR8V_nIaN-R",
    "outputId": "4271125f-67a2-44e6-bc52-e635c369e71b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (4.24.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: filelock in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: requests in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/arjun/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2022.5.18.1)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "!pip install transformers\n",
    "from transformers import BertModel, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "QSF73dasaV3j"
   },
   "outputs": [],
   "source": [
    "class Incep3Encoder(torch.nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(Incep3Encoder, self).__init__()\n",
    "\n",
    "        self.incep3 = torchvision.models.inception_v3(pretrained=True)\n",
    "        self.incep3.aux_logits = False # We don't use the auxiliary output.\n",
    "        for parameter in self.incep3.parameters():\n",
    "            parameter.requires_grad = False # Freeze all the parameters.\n",
    "        self.incep3.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.incep3.fc.in_features, 512), # in_features=2048 in Inception V3.\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, embed_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.incep3(x)\n",
    "\n",
    "class BertEncoder(torch.nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(BertEncoder, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=False) # With bert-base-uncased, all the input are lowercased before being tokenized.\n",
    "        for parameter in self.bert.parameters():\n",
    "            parameter.requires_grad = False # Freeze all the parameters.\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.bert.config.hidden_size, 512), # hidden_size=768.\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, embed_dim),\n",
    "            torch.nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        last_hidden, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
    "        return self.fc(last_hidden[:, 0])\n",
    "\n",
    "def Encoder(name, embed_dim=128):\n",
    "    if name == \"InceptionV3\":\n",
    "        return Incep3Encoder(embed_dim)\n",
    "    elif name == \"BERT\":\n",
    "        return BertEncoder(embed_dim)\n",
    "    elif name == \"YOLO\":\n",
    "        return yoloE.EncoderModel()\n",
    "    else:\n",
    "        raise ValueError(name + \" has not been implemented!\")\n",
    "\n",
    "def printParameters(encoder=\"all\"):\n",
    "    print(\"\\n\")\n",
    "    if encoder in [\"image\", \"all\"]:\n",
    "        image_enc_total_params = 0\n",
    "        image_enc_trainable_params = 0\n",
    "        for parameter in image_enc.parameters():\n",
    "            image_enc_total_params += np.prod(parameter.size())\n",
    "            if parameter.requires_grad:\n",
    "                image_enc_trainable_params += np.prod(parameter.size())\n",
    "        print(f\"=== Image Encoder ===\")\n",
    "        print(f\"  Total Parameters: {image_enc_total_params:,}\")\n",
    "        print(f\"  Trainable Parameters: {image_enc_trainable_params:,}\\n\")\n",
    "    if encoder in [\"text\", \"all\"]:\n",
    "        text_enc_total_params = 0\n",
    "        text_enc_trainable_params = 0\n",
    "        for parameter in text_enc.parameters():\n",
    "            text_enc_total_params += np.prod(parameter.size())\n",
    "            if parameter.requires_grad:\n",
    "                text_enc_trainable_params += np.prod(parameter.size())\n",
    "        print(f\"=== Text Encoder ===\")\n",
    "        print(f\"  Total Parameters: {text_enc_total_params:,}\")\n",
    "        print(f\"  Trainable Parameters: {text_enc_trainable_params:,}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "gP_ioNk9tZMW"
   },
   "outputs": [],
   "source": [
    "class ImagePreprocessor():\n",
    "    def process(self, x):\n",
    "        return torch.swapaxes(torch.swapaxes(x, 3, 2), 2, 1)\n",
    "\n",
    "class BertPreprocessor():\n",
    "    def __init__(self, max_len=50):\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", model_max_length=self.max_len)\n",
    "    \n",
    "    def process(self, x):\n",
    "        padded_list = self.tokenizer.batch_encode_plus(x, padding=True)\n",
    "        input_ids = torch.LongTensor(np.array(padded_list[\"input_ids\"])) # It looks like bypassing numpy is faster.\n",
    "        attention_mask = torch.LongTensor(np.array(padded_list[\"attention_mask\"]))\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "def Preprocessor(name, max_len=50):\n",
    "    if name in [\"InceptionV3\", \"YOLO\"]:\n",
    "        return ImagePreprocessor()\n",
    "    elif name in [\"BERT\"]:\n",
    "        return BertPreprocessor(max_len)\n",
    "    else:\n",
    "        raise ValueError(name + \" has not been implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-IFKlBHnf8T",
    "outputId": "e48b71af-ece4-41dc-a549-bd9ca888c3e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1    229245  yolov5.models.yoloE.Detect              [80, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7235389 parameters, 7235389 gradients, 16.6 GFLOPs\n",
      "\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Image Encoder ===\n",
      "  Total Parameters: 69,436,140\n",
      "  Trainable Parameters: 69,436,140\n",
      "\n",
      "=== Text Encoder ===\n",
      "  Total Parameters: 110,029,868\n",
      "  Trainable Parameters: 547,628\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 300\n",
    "image_model = \"YOLO\"\n",
    "text_model = \"BERT\"\n",
    "\n",
    "image_enc = Encoder(image_model, embed_dim).to(device)\n",
    "text_enc = Encoder(text_model, embed_dim).to(device)\n",
    "printParameters()\n",
    "\n",
    "image_proc = Preprocessor(image_model)\n",
    "text_proc = Preprocessor(text_model, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "i-O5O-dXrLoX"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CosineEmbeddingLoss(margin=0.1) # Recipe 1M+ paper sets margin=0.1.\n",
    "image_optimizer = torch.optim.Adam(image_enc.parameters(), lr=0.001)\n",
    "text_optimizer = torch.optim.Adam(text_enc.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "OQ1VuVHbEGZE"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    image_enc.train()\n",
    "    text_enc.train()\n",
    "    train_loss = 0\n",
    "    n_samples = 0\n",
    "\n",
    "    for batch in trainloader:\n",
    "        print(batch[\"images\"].type())\n",
    "        image_batch = image_proc.process(batch[\"images\"]).to(device).type(torch.FloatTensor)\n",
    "        text_batch, attention_batch = text_proc.process(batch[\"texts\"])\n",
    "        text_batch = text_batch.to(device)\n",
    "        if attention_batch is not None:\n",
    "            attention_batch = attention_batch.to(device)\n",
    "        label_batch = torch.LongTensor(np.array(batch[\"labels\"])).to(device)\n",
    "\n",
    "        image_optimizer.zero_grad()\n",
    "        text_optimizer.zero_grad()\n",
    "        print(image_batch.type())\n",
    "        image_embedding = image_enc(image_batch)\n",
    "        if attention_batch is not None:\n",
    "            text_embedding = text_enc(text_batch, attention_batch)\n",
    "        else:\n",
    "            text_embedding = text_enc(text_batch)\n",
    "        loss = criterion(image_embedding, text_embedding, label_batch)\n",
    "        loss.backward()\n",
    "        image_optimizer.step()\n",
    "        text_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        n_samples += label_batch.size(0)\n",
    "\n",
    "    train_loss /= n_samples\n",
    "    return train_loss\n",
    "\n",
    "def eval():\n",
    "    image_enc.eval()\n",
    "    text_enc.eval()\n",
    "    eval_loss = 0\n",
    "    n_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            image_batch = image_proc.process(batch[\"images\"]).to(device)\n",
    "            text_batch, attention_batch = text_proc.process(batch[\"texts\"])\n",
    "            text_batch = text_batch.to(device)\n",
    "            if attention_batch is not None:\n",
    "                attention_batch = attention_batch.to(device)\n",
    "            label_batch = torch.LongTensor(np.array(batch[\"labels\"])).to(device)\n",
    "\n",
    "            image_embedding = image_enc(image_batch)\n",
    "            if attention_batch is not None:\n",
    "                text_embedding = text_enc(text_batch, attention_batch)\n",
    "            else:\n",
    "                text_embedding = text_enc(text_batch)\n",
    "            loss = criterion(image_embedding, text_embedding, label_batch)\n",
    "\n",
    "            eval_loss += loss\n",
    "            n_samples += label_batch\n",
    "\n",
    "    eval_loss /= n_samples\n",
    "    return eval_loss\n",
    "\n",
    "def test():\n",
    "    image_enc.eval()\n",
    "    text_enc.eval()\n",
    "    image_mapped = []\n",
    "    # Under construction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 433
    },
    "id": "dCKngBdHCwvu",
    "outputId": "803e8634-9e33-4ec0-a4c1-cefd466c3379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training...\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n",
      "torch.Size([128, 512, 10, 10])\n",
      "torch.ByteTensor\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [48], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     10\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(DualEncoderDataset(train_images, train_texts, neg_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m), \n\u001b[1;32m     11\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[1;32m     12\u001b[0m                          shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn [47], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m text_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_batch\u001b[38;5;241m.\u001b[39mtype())\n\u001b[0;32m---> 19\u001b[0m image_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mimage_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m text_enc(text_batch, attention_batch)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Academics/Semester 3/DL/Group Project/video-search-dl/yolov5/models/yoloE.py:113\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, profile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, visualize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Academics/Semester 3/DL/Group Project/video-search-dl/yolov5/models/yoloE.py:122\u001b[0m, in \u001b[0;36mBaseModel._forward_once\u001b[0;34m(self, x, profile, visualize)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[0;32m--> 122\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[1;32m    123\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Academics/Semester 3/DL/Group Project/video-search-dl/yolov5/models/common.py:168\u001b[0m, in \u001b[0;36mC3.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/Academics/Semester 3/DL/Group Project/video-search-dl/yolov5/models/common.py:57\u001b[0m, in \u001b[0;36mConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/activation.py:391\u001b[0m, in \u001b[0;36mSiLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:2047\u001b[0m, in \u001b[0;36msilu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   2045\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   2046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 2047\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39msilu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "train_loss = []\n",
    "\n",
    "print(\"Start Training...\")\n",
    "for epoch in range(epochs):\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(\"\\nEpoch: \" + str(epoch + 1))\n",
    "    trainloader = DataLoader(DualEncoderDataset(train_images, train_texts, neg_rate=2), \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True)\n",
    "    loss = train()\n",
    "    train_loss.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VyZA-T5AOqCY"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(np.arange(1, epochs + 1), train_loss)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9STcV0r1aRZu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
