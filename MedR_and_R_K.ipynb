{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CSGY 6953 Deep Learning Final Project\n",
        "In this project, we will implement a dual-encoder model for image search. In particular, our model will be fed a text query and will return several images that are related to the query. To do this task, our model will be trained so that it embeds both image and text data into the same space, and importantly, encodes relevant data to be close each other in the embedding space. This will be done by developing two encoders, one for image processing and the other for text encoding, and training them by a similarity-based loss function. <br>\n",
        "<br>\n",
        "In this notebook, I develop the evaluation metrics, median rank (**MedR**) and recall rate at top K (**R@K**). <br>\n",
        "<br>\n",
        "In the test phase, we first project all the test images into the embedding space via the image encoder. For each test (image, text) pair, we consider the text as a query, which is also mapped into the embedding space through the text encoder. Then, we rank all the test images by the similarity to the query. The **MedR** is the median of the rank of the true image over all the test data. So, smaller **MedR** indicates that the model works well. In **R@K**, we take only top K images for each query and check if the true image is included. By doing this check for all test data, we can calculate the probability that the true image is successfully retrieved. Thus, higher **R@K** is better."
      ],
      "metadata": {
        "id": "IJhTIUkxV-yW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intuition behind the CosineEmbeddingLoss\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.CosineEmbeddingLoss.html<br>\n",
        "$$\\text{loss}(x,y) = \\begin{cases} 1-\\cos(x_{image}, x_{text}) & \\text{if } y=1 \\\\ \\max(0, \\cos(x_{image}, x_{text})- \\text{margin}) & \\text{if } y=-1 \\end{cases}$$\n",
        "Let $d$ be the dimensionality of the embedding space. When negative pair $(x^-, -1)$ is given, we want to map $x_{image}, x_{text}$ in distant place. One might think that having $\\cos(x_{image}, x_{text}) = -1$ is ideal, but given $x_{image}$, there is only one vector (up to scaling) in the embedding space that satisfies it. Since we have many vectors that are separated each other, forcing all the pairs of them having cosine similarity (nearly) $-1$ is impossible. <br>\n",
        "By contrast, forcing them having cosine similarity (nearly) $0$ is possible. In other words, forcing them being (nearly) orthogonal is possible. Nearly orthogonal is defined as the normalized dot product is $0$ with a small error term $\\epsilon > 0$. That is, \n",
        "$$-\\epsilon \\leq \\frac{\\langle x_i, x_j \\rangle}{\\lVert x_i \\rVert_2 \\lVert x_j \\rVert_2} \\leq \\epsilon$$\n",
        "While there are only $d$ mutually orthogonal vectors in $d$ dimensional space, there exists $2^{\\Theta(\\epsilon^2 d)}$ mutually nearly orthogonal vectors. Thus, our goal is to train encoders so that negative inputs are mapped as nearly orthogonal. In this context, it makes sense not to penalize so long as cosine similarity is negative, and moreover, allow a small margin to let vectors nearly orthogonal, not strictly orthogonal. <br>\n",
        "Note that for positive pairs, this loss function enforces them to having cosine similarity $1$."
      ],
      "metadata": {
        "id": "_yx06Sfl22Hc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W1jCGXeDVVSM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import random\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebQuixn6ZKEk",
        "outputId": "0a93a315-ee70-4e0f-ff86-646787d464c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f\"drive/My Drive/finalproj/data/dataset.pkl\", \"rb\") as f:\n",
        "    dataset = pickle.load(f)"
      ],
      "metadata": {
        "id": "RiD9kidTZLVH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_len = [len(s.split()) for s in dataset[\"texts\"]]\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.xlabel(\"caption length\")\n",
        "plt.ylabel(\"# captions\")\n",
        "plt.hist(text_len)\n",
        "plt.show()\n",
        "print(\"Shortest caption length: \" + str(min(text_len)))\n",
        "print(\"Longest caption length: \" + str(max(text_len)))\n",
        "\n",
        "max_len = 55 # BERT tokenizer is not one word per one embedding.\n",
        "print(\"The maximum sentence length for the text encoder is set to \" + str(max_len) + \".\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "C0OMHApOozeC",
        "outputId": "042a5448-1a6d-4e32-dd2d-52be90b45801"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x216 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shortest caption length: 6\n",
            "Longest caption length: 46\n",
            "The maximum sentence length for the text encoder is set to 46.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pct = 0.7\n",
        "threshold = int(len(dataset[\"images\"]) * train_pct)\n",
        "train_images = dataset[\"images\"][:threshold]\n",
        "train_texts = dataset[\"texts\"][:threshold * 10]\n",
        "test_images = dataset[\"images\"][threshold:]\n",
        "test_texts = dataset[\"texts\"][threshold * 10:]\n",
        "print(f\"Training images: \", len(train_images))\n",
        "print(f\"Training texts: \", len(train_texts))\n",
        "print(f\"Test images: \", len(test_images))\n",
        "print(f\"Test texts: \", len(test_texts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgDpODirZScy",
        "outputId": "63f980e3-85f9-4075-888a-f81e8b68c185"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images:  3150\n",
            "Training texts:  31500\n",
            "Test images:  1350\n",
            "Test texts:  13500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "kaBGEWLpZTsi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "\n",
        "class DualEncoderDataset(Dataset):\n",
        "    def __init__(self, images, texts, neg_rate=4):\n",
        "        # This class assumes that num_image < num_texts, and the number of texts per image is the same for all images.\n",
        "        self.imagedata = images\n",
        "        self.textdata = texts\n",
        "        self.num_images = len(images)\n",
        "        self.num_texts = len(texts)\n",
        "        self.image_text_rate = int(self.num_texts / self.num_images)\n",
        "        self.num_negative = int(self.num_texts * neg_rate)\n",
        "        self.random_map = []\n",
        "        self.shuffle()\n",
        "    \n",
        "    def shuffle(self):\n",
        "        self.random_map = np.random.randint(0, self.num_texts - self.image_text_rate, self.num_negative)\n",
        "        for i in range(self.num_negative):\n",
        "            if self.random_map[i] >= i % self.num_texts // self.image_text_rate * self.image_text_rate:\n",
        "                self.random_map[i] += self.image_text_rate\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.num_texts + self.num_negative\n",
        "\n",
        "    def __getitem__(self, sample_idx):\n",
        "        if torch.is_tensor(sample_idx):\n",
        "            sample_idx = sample_idx[0].tolist()\n",
        "        image_idx = sample_idx % self.num_texts // self.image_text_rate\n",
        "        if sample_idx < self.num_texts:\n",
        "            text_idx = sample_idx\n",
        "        else:\n",
        "            text_idx = self.random_map[sample_idx - self.num_texts]\n",
        "        sample = {\"images\": self.imagedata[image_idx], \"texts\": self.textdata[text_idx], \"labels\": int(sample_idx < self.num_texts) * 2 - 1}\n",
        "        return sample"
      ],
      "metadata": {
        "id": "wv2zQDXTZYpi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"The device is \" + device.type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJvcxfAnZcWC",
        "outputId": "64977934-e299-4efb-a421-09736151f2cd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "!pip install transformers\n",
        "from transformers import BertModel, BertTokenizerFast"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvR8V_nIaN-R",
        "outputId": "4271125f-67a2-44e6-bc52-e635c369e71b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 46.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 77.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Incep3Encoder(torch.nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(Incep3Encoder, self).__init__()\n",
        "\n",
        "        self.incep3 = torchvision.models.inception_v3(pretrained=True)\n",
        "        self.incep3.aux_logits = False # We don't use the auxiliary output.\n",
        "        for parameter in self.incep3.parameters():\n",
        "            parameter.requires_grad = False # Freeze all the parameters.\n",
        "        self.incep3.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.incep3.fc.in_features, 512), # in_features=2048 in Inception V3.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, embed_dim),\n",
        "            torch.nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.incep3(x)\n",
        "\n",
        "class BertEncoder(torch.nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super(BertEncoder, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=False) # With bert-base-uncased, all the input are lowercased before being tokenized.\n",
        "        for parameter in self.bert.parameters():\n",
        "            parameter.requires_grad = False # Freeze all the parameters.\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.bert.config.hidden_size, 512), # hidden_size=768.\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, embed_dim),\n",
        "            torch.nn.Tanh()\n",
        "        )\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        last_hidden, _ = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=False)\n",
        "        return self.fc(last_hidden[:, 0])\n",
        "\n",
        "def Encoder(name, embed_dim=128):\n",
        "    if name == \"InceptionV3\":\n",
        "        return Incep3Encoder(embed_dim)\n",
        "    elif name == \"BERT\":\n",
        "        return BertEncoder(embed_dim)\n",
        "    else:\n",
        "        raise ValueError(name + \" has not been implemented!\")\n",
        "\n",
        "def printParameters(encoder=\"all\"):\n",
        "    print(\"\\n\")\n",
        "    if encoder in [\"image\", \"all\"]:\n",
        "        image_enc_total_params = 0\n",
        "        image_enc_trainable_params = 0\n",
        "        for parameter in image_enc.parameters():\n",
        "            image_enc_total_params += np.prod(parameter.size())\n",
        "            if parameter.requires_grad:\n",
        "                image_enc_trainable_params += np.prod(parameter.size())\n",
        "        print(f\"=== Image Encoder ===\")\n",
        "        print(f\"  Total Parameters: {image_enc_total_params:,}\")\n",
        "        print(f\"  Trainable Parameters: {image_enc_trainable_params:,}\\n\")\n",
        "    if encoder in [\"text\", \"all\"]:\n",
        "        text_enc_total_params = 0\n",
        "        text_enc_trainable_params = 0\n",
        "        for parameter in text_enc.parameters():\n",
        "            text_enc_total_params += np.prod(parameter.size())\n",
        "            if parameter.requires_grad:\n",
        "                text_enc_trainable_params += np.prod(parameter.size())\n",
        "        print(f\"=== Text Encoder ===\")\n",
        "        print(f\"  Total Parameters: {text_enc_total_params:,}\")\n",
        "        print(f\"  Trainable Parameters: {text_enc_trainable_params:,}\\n\")"
      ],
      "metadata": {
        "id": "QSF73dasaV3j"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImagePreprocessor():\n",
        "    def process(self, x):\n",
        "        return torch.swapaxes(torch.swapaxes(x, 3, 2), 2, 1)\n",
        "\n",
        "class BertPreprocessor():\n",
        "    def __init__(self, max_len=50):\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", model_max_length=self.max_len)\n",
        "    \n",
        "    def process(self, x):\n",
        "        padded_list = self.tokenizer.batch_encode_plus(x, padding=True)\n",
        "        input_ids = torch.LongTensor(np.array(padded_list[\"input_ids\"])) # It looks like bypassing numpy is faster.\n",
        "        attention_mask = torch.LongTensor(np.array(padded_list[\"attention_mask\"]))\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "def Preprocessor(name, max_len=50):\n",
        "    if name in [\"InceptionV3\"]:\n",
        "        return ImagePreprocessor()\n",
        "    elif name in [\"BERT\"]:\n",
        "        return BertPreprocessor(max_len)\n",
        "    else:\n",
        "        raise ValueError(name + \" has not been implemented!\")"
      ],
      "metadata": {
        "id": "gP_ioNk9tZMW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 100\n",
        "image_model = \"InceptionV3\"\n",
        "text_model = \"BERT\"\n",
        "\n",
        "image_enc = Encoder(image_model, embed_dim).to(device)\n",
        "text_enc = Encoder(text_model, embed_dim).to(device)\n",
        "printParameters()\n",
        "\n",
        "image_proc = Preprocessor(image_model)\n",
        "text_proc = Preprocessor(text_model, max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-IFKlBHnf8T",
        "outputId": "e48b71af-ece4-41dc-a549-bd9ca888c3e7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "=== Image Encoder ===\n",
            "  Total Parameters: 26,212,652\n",
            "  Trainable Parameters: 1,100,388\n",
            "\n",
            "=== Text Encoder ===\n",
            "  Total Parameters: 109,927,268\n",
            "  Trainable Parameters: 445,028\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CosineEmbeddingLoss(margin=0.1) # Recipe 1M+ paper sets margin=0.1.\n",
        "image_optimizer = torch.optim.Adam(image_enc.parameters(), lr=0.001)\n",
        "text_optimizer = torch.optim.Adam(text_enc.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "i-O5O-dXrLoX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    image_enc.train()\n",
        "    text_enc.train()\n",
        "    train_loss = 0\n",
        "    n_samples = 0\n",
        "\n",
        "    for batch in trainloader:\n",
        "        image_batch = image_proc.process(batch[\"images\"]).to(device)\n",
        "        text_batch, attention_batch = text_proc.process(batch[\"texts\"])\n",
        "        text_batch = text_batch.to(device)\n",
        "        if attention_batch is not None:\n",
        "            attention_batch = attention_batch.to(device)\n",
        "        label_batch = torch.LongTensor(np.array(batch[\"labels\"])).to(device)\n",
        "\n",
        "        image_optimizer.zero_grad()\n",
        "        text_optimizer.zero_grad()\n",
        "        image_embedding = image_enc(image_batch)\n",
        "        if attention_batch is not None:\n",
        "            text_embedding = text_enc(text_batch, attention_batch)\n",
        "        else:\n",
        "            text_embedding = text_enc(text_batch)\n",
        "        loss = criterion(image_embedding, text_embedding, label_batch)\n",
        "        loss.backward()\n",
        "        image_optimizer.step()\n",
        "        text_optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        n_samples += label_batch.size(0)\n",
        "\n",
        "    train_loss /= n_samples\n",
        "    return train_loss\n",
        "\n",
        "def eval():\n",
        "    image_enc.eval()\n",
        "    text_enc.eval()\n",
        "    eval_loss = 0\n",
        "    n_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in testloader:\n",
        "            image_batch = image_proc.process(batch[\"images\"]).to(device)\n",
        "            text_batch, attention_batch = text_proc.process(batch[\"texts\"])\n",
        "            text_batch = text_batch.to(device)\n",
        "            if attention_batch is not None:\n",
        "                attention_batch = attention_batch.to(device)\n",
        "            label_batch = torch.LongTensor(np.array(batch[\"labels\"])).to(device)\n",
        "\n",
        "            image_embedding = image_enc(image_batch)\n",
        "            if attention_batch is not None:\n",
        "                text_embedding = text_enc(text_batch, attention_batch)\n",
        "            else:\n",
        "                text_embedding = text_enc(text_batch)\n",
        "            loss = criterion(image_embedding, text_embedding, label_batch)\n",
        "\n",
        "            eval_loss += loss\n",
        "            n_samples += label_batch\n",
        "\n",
        "    eval_loss /= n_samples\n",
        "    return eval_loss\n",
        "\n",
        "def test():\n",
        "    image_enc.eval()\n",
        "    text_enc.eval()\n",
        "    image_mapped = []\n",
        "    # Under construction!"
      ],
      "metadata": {
        "id": "OQ1VuVHbEGZE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 128\n",
        "\n",
        "train_loss = []\n",
        "\n",
        "print(\"Start Training...\")\n",
        "for epoch in range(epochs):\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        print(\"\\nEpoch: \" + str(epoch + 1))\n",
        "    trainloader = DataLoader(DualEncoderDataset(train_images, train_texts, neg_rate=2), \n",
        "                             batch_size=batch_size, \n",
        "                             shuffle=True)\n",
        "    loss = train()\n",
        "    train_loss.append(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "dCKngBdHCwvu",
        "outputId": "803e8634-9e33-4ec0-a4c1-cefd466c3379"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (53 > 46). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch: 2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ac1e88cee8d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                              shuffle=True)\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-2f345761fe00>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mtext_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mimage_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtext_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(np.arange(1, epochs + 1), train_loss)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VyZA-T5AOqCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9STcV0r1aRZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}